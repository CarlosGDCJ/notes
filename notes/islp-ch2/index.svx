---
title: ISLP - Chapter 2
date: 10 June 2023
tags:
    - ISLP
    - ISL
    - Python
---

How hard could it be?

## Notation

* $n$: number of *data points*, *observations*
* $p$: number of *variables*, *features*
* $X$: Matrix of *inputs, predictors, independent variables, features*
    * $x_i$: row of the matrix, with data corresponding to one observation (lower case normal font)
    * $\bold{x}_j$: column of the matrix with data corresponding to one variable (lower case bold)

$$
X=\begin{pmatrix}
x_{11} & x_{12} & \dots & x_{1p}\\
x_{21} & x_{22} & \dots & x_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & \dots & x_{np}\\
\end{pmatrix}
$$
* $Y$: *output, response, dependent variable*


## Statistical Learning
**Statistical Learning** is the set of approaches used to approximate the function
$f$
$$
Y = f(X) + \epsilon
$$

$f$ represents the *systematic* information $X$ provides about $Y$, and that's what we
try to estimate. $f$'s estimate is $\hat{f}$, and that can be used to predict $Y$.
$$
\hat{Y} = \hat{f}(X)
$$

$\epsilon$ is an *error term* independent of $X$ and has mean zero. We have no
hope of getting this right(is independent of X), and as it averages to zero,
we don't bother. This term represents things we didn't consider, i.e. unmeasured
variables and unmeasured variation.

### Prediction vs Inference
When doing **prediction**, we treat $\hat{f}$ as a black box and are only concerned
with the outcomes of said box. When doing **inference** we often want to know the
form of $\hat{f}$ to uncover relationships between it and $X$.

## Types of learning methods
### Parametric
Siplifies the problem of trying to estimate $f$ by assuming a functional form ("shape")
for $f$ and trying to find out the correct coefficients using the training data $X$.

### Non-parametric
Makes no assumption on the functional form of $f$, which avoids the problem of choosing
a form that's far from the truth, but need a lot of observations to create an accurate
estimate.

### Supervised
The response variables are available

### Unsupervised
The response variables are not available. The task then becomes to find relationships
between the variables or between observations

### Semi-Supervised
The response varialbe is avaliable only for some of the observations

### Regression
The response variable is **quantitative** (a number)

### Classification
The response variable is **qualitative** (a value from a set of values)


## Evaluating learning methods
When evaluating a method, we are only interested in how the method performs when
dealing with unseen observations. Most statistical methods either directly or
indirectly try to do well on the training data.

$(x_0, y_0)$: Notation for a previously unseen test observation not used to train the
learning method

There is no guarantee that a good score when training reflects on a good score when
when testing. As model flexibility increases, performance on training data increases,
but nothing can be said about test data.

As it becomes more flexible, the statistical learning procedure can try to find
patterns in the training data that are not inherent characteristics of the data, but
are just random noise.

## Bias-variance tradeoff
The expected MSE error of a statistical learning model on unseen data can be
decomposed as follows:

$$
E(y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon)
$$

* $Var(\epsilon)$ is associated with the *irreducible error*, and test MSE can never
fall below this value.
* $Var(\hat{f}(x_0))$ is related to the amount $\hat{f}$ would change if estimated
using different training data. Highly flexible models tend to have $\hat{f}$s that
vary wildly.
* $[Bias(\hat{f}(x_0))]^2$ is the error term associated with using a simplified
model to estimate $f$, when $f$ can be completely different. Like using linear
regression when $f$ is non-linear.

Normally, with increased flexibility/complexity, you can decrease bias, but by increasing
flexibility you increase variance as well.