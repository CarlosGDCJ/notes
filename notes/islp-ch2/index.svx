---
title: ISLP - Chapter 2
date: 10 June 2023
tags:
    - ISLP
    - ISL
    - Python
---

How hard could it be?

## Notation

* $n$: number of *data points*, *observations*
* $p$: number of *variables*, *features*
* $X$: Matrix of *inputs, predictors, independent variables, features*
    * $x_i$: row of the matrix, with data corresponding to one observation (lower case normal font)
    * $\bold{x}_j$: column of the matrix with data corresponding to one variable (lower case bold)

$$
X=\begin{pmatrix}
x_{11} & x_{12} & \dots & x_{1p}\\
x_{21} & x_{22} & \dots & x_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & \dots & x_{np}\\
\end{pmatrix}
$$
* $Y$: *output, response, dependent variable*


## Statistical Learning
**Statistical Learning** is the set of approaches used to approximate the function
$f$
$$
Y = f(X) + \epsilon
$$

$f$ represents the *systematic* information $X$ provides about $Y$, and that's what we
try to estimate. $f$'s estimate is $\hat{f}$, and that can be used to predict $Y$.
$$
\hat{Y} = \hat{f}(X)
$$

$\epsilon$ is an *error term* independent of $X$ and has mean zero. We have no
hope of getting this right(is independent of X), and as it averages to zero,
we don't bother. This term represents things we didn't consider, i.e. unmeasured
variables and unmeasured variation.

### Prediction vs Inference
When doing **prediction**, we treat $\hat{f}$ as a black box and are only concerned
with the outcomes of said box. When doing **inference** we often want to know the
form of $\hat{f}$ to uncover relationships between it and $X$.

## Types of learning methods
### Parametric
Siplifies the problem of trying to estimate $f$ by assuming a functional form ("shape")
for $f$ and trying to find out the correct coefficients using the training data $X$.

### Non-parametric
Makes no assumption on the functional form of $f$, which avoids the problem of choosing
a form that's far from the truth, but need a lot of observations to create an accurate
estimate.

### Supervised
The response variables are available

### Unsupervised
The response variables are not available. The task then becomes to find relationships
between the variables or between observations

### Semi-Supervised
The response varialbe is avaliable only for some of the observations

### Regression
The response variable is **quantitative** (a number)

### Classification
The response variable is **qualitative** (a value from a set of values)


## Evaluating learning methods
When evaluating a method, we are only interested in how the method performs when
dealing with unseen observations. Most statistical methods either directly or
indirectly try to do well on the training data.

$(x_0, y_0)$: Notation for a previously unseen test observation not used to train the
learning method

There is no guarantee that a good score when training reflects on a good score when
when testing. As model flexibility increases, performance on training data increases,
but nothing can be said about test data.

As it becomes more flexible, the statistical learning procedure can try to find
patterns in the training data that are not inherent characteristics of the data, but
are just random noise.

## Bias-variance tradeoff
The expected MSE error of a statistical learning model on unseen data can be
decomposed as follows:

$$
E(y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon)
$$

* $Var(\epsilon)$ is associated with the *irreducible error*, and test MSE can never
fall below this value.
* $Var(\hat{f}(x_0))$ is related to the amount $\hat{f}$ would change if estimated
using different training data. Highly flexible models tend to have $\hat{f}$s that
vary wildly.
* $[Bias(\hat{f}(x_0))]^2$ is the error term associated with using a simplified
model to estimate $f$, when $f$ can be completely different. Like using linear
regression when $f$ is non-linear.

Normally, with increased flexibility/complexity, you can decrease bias, but by
increasing flexibility you increase variance as well. Choosing the correct level
of flexibility is critical to the success of any statistical learning method. **This**
**concept transfer over to the classification setting as well.**

## Conceptual Exercises
$1$

$a)$ Flexible methods would perform better, as we have a lot of data points to work
with and these kinds of methods do well with an abundance of data

$b)$ Inflexible methods would perform better. The limited amount of data makes it
more likely that a flexible model will learn characteristics that are specific for
the training data, and not about the true $f$.

$c)$ Flexible methods would perform better, as they generally don't make as many
assumptions about the form of $f$. Inflexible methods, on the other hand, often
simplify $f$ by assuming a functional form, and in the case of a highly non-linear
$f$, it would be quite detrimental

$d)$ Inflexible methods would perform better. A high variance on the error term would
make for highly different training sets. In this setting, flexible methods are prone
to learn from the noise instead of the signal $f$.

$2$

$a)$ It's a **regression** problem and we are mostly interested in **inference**. 
The goal is to understand how $X$ affects the target $Y$, and the variable of interest
is quantitative.
$$
n = 500\\
p = 4
$$

$b)$ It's a **classification** problem and we are mostly interested in **prediction**. 
We are only concerned if the outcome is success or not, and the variable of interest
is qualitative.
$$
n = 20\\
p = 14
$$

$c)$  It's a **regression** problem and we are mostly interested in **prediction**.
We are only concerned in finding out the value of the % change in USD/Euro, a
quantitative variable
$$
n = numWeeksIn2012\\
p = 4
$$

$3$

$a)$ Soon<sup>TM</sup> I'll create a graph for this.

$b)$
* **Variance** starts small, but increases with the flexibility
* **Bias** starts higher, but decreases with the flexibility
* **Bayes error** is a straight line that serves as a lower limit for the test error
* **Test error** is the addition of the three previous lines, which gives it the U shape:
starts high because of high bias, ends high because of high variance
* **Train error** goes down with flexibility, being able to go lower than Bayes error
and even reach 0

$5$

More flexible approaches are better suited to handle complex relationships between
the independent variables and the response. They are also more useful to the prediction
task, as their results are hard to interpret (black box).

If the relationship between predictors and the dependent variable is not expected to be
complex, or the main interest in to understand the relationship between them,
less flexible approaches are more suited

$6$

A parametric approach makes assumptions about the functional form of $f$, where $f$ is
the function we are trying to estimate. It effectively simplifies the problem.

A non-parametric approach makes no such assumptions, but in exchange needs, in general,
more data to come up with a reasonable estimate of $f$.

$7$

| Obs. | X<sub>1</sub> | X<sub>2</sub> | X<sub>3</sub> | Y | Dist to Origin |
|:-:|:-:|:-:|:-:|:-:|:-:|
1|0|3|0|Red|$\sqrt{9}$|
2|2|0|0|Red|$\sqrt{4}$|
3|0|1|3|Red|$\sqrt{10}$|
4|0|1|2|Green|$\sqrt{5}$|
5|-1|0|1|Green|$\sqrt{2}$|
6|1|1|1|Red|$\sqrt{3}$|

$a)$ See **Dist to Origin** column

$b)$ Nearest Neighbor would be **Obs. 5**, the new element gets classified as Green

$c)$ Nearest Neighbors would be **Obs. 5, Obs. 6, Obs. 2**, the new element gets
classified as Red

$d)$ Non-linearity usually calls for an increased flexibility. When dealing with the
KNN algorithm, flexibility is increased by a lower $K$, so $K$ is expected to be small.
Also, in this case, as we're dealing with few data points, any large $K$ would mostly
yield the classification "Red"